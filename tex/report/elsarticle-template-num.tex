%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
%\documentclass[preprint,12pt]{elsarticle}

\documentclass[final,5p,times,twocolumn]{elsarticle}

\usepackage{amssymb}
\usepackage{parskip}

\begin{document}

\begin{frontmatter}

\title{Final Report}
\author{Daniel Yao}

\begin{abstract}
Lorem ipsum dolor sit amet, consectetur adipiscing elit.
\end{abstract}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}

Risk \cite{KepplerChoi2000Risk}

\textbf{Def.} A finite Markov Decision Process (MDP) is a five-tuple $(S, A, P, R, \gamma)$ where\cite{risk1959}\cite{puterman2014markov}
\begin{enumerate}
    \item $S$ is the finite state space,
    \item $A(s)$ is the finite action space for state $s \in S$,
    \item $P: S \times A \times S$ is the transition probability function,
    \item $R: S \times A \times S$ is the reward function, and 
    \item $\gamma \in [0, 1]$ is the discount factor.
\end{enumerate}
$P(s' \mid s, a)$ is the probability that the next state is $s' \in S$ given that the current state is $s \in S$ and the action taken is $a \in A(s)$. $R(s', a, s)$ is the reward received when the current state is $s' \in S$, the action taken was $a \in A(s)$, and the previous state was $s \in S$.

\textbf{Def.} A policy $\pi$ is a function $\pi: A \times S \to [0, 1]$ where $\pi(a \mid s)$ is the probability that an agent in state $s \in S$ takes action $a \in A(s)$. This is a probability distribution, so 
$$\sum_{a \in A(s)} \pi(a \mid s) = 1$$
for all $s \in S$.

\textbf{Def.} The discounted return $G_{t}$ at time $t$ is the sum of all future rewards, discounted by the factor $\gamma$. That is,
$$G_{t} = \sum_{k=1}^{\infty} \gamma^{k} R_{t+k}$$
where $R_{t+k}$ is the reward received at time $t+k$. 

\textbf{Def.} The state-value function $V_{\pi}(s)$ is the expected return when starting in state $s$ and following policy $\pi$:

\section{Markov Decision Process}
\label{sec:mdp}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.

\section{Reinforcement Learning}
\label{sec:rl}

\section{Simulation Study}
\label{sec:sim}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.

\section{Discussion}
\label{sec:disc}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.

\section{Conclusion}
\label{sec:concl}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.

\bibliographystyle{elsarticle-num} 
\bibliography{cas-refs}

\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
