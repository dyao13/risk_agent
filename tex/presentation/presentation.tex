\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bookmark}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{hyperref}

\title[MDP]{Markov Decision Processes: \\An Application to RISK}
\author{Daniel Yao}
\institute{Johns Hopkins University}
\date{24 April 2025}

%%%

\begin{document}

\begin{frame}

\titlepage

\end{frame}

%%

\begin{frame}{Introduction}

\begin{block}{Markov Decision Process \cite{puterman2014markov}} 
\begin{itemize}
\item A \textit{Markov Decision Process} (MDP) is a tuple ($S$, $A$, $P$, $R$, $\gamma$).
\item $S$ is the \textit{state space}.
\item $A$ is the \textit{action space} where $A(s)$ is the action space of state $s$.
\item $P: S \times A \times S \to [0, 1]$ is the \textit{transition function} where $P(s' \mid s, a)$ is the probability of transitioning to state $s'$ given that action $a$ is taken in state $s$.
\item $R: S \times A \times S \to \mathbb{R}$ is the \textit{reward function} where $R(s, a, s')$ is the reward received after transitioning to state $s'$ from state $s$ by taking action $a$.
\item $\gamma \in [0, 1]$ is the \textit{discount factor}.
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{Introduction}

\begin{block}{Markov Property \cite{puterman2014markov}} 
\begin{itemize}
\item An MDP has the \textit{Markov property} that 
$$P(s_{t+1} \mid s_{t}, a_{t}, s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1} \mid s_{t}, a_{t})$$
\end{itemize}
\end{block}

\begin{block}{Transition Diagram \cite{puterman2014markov}}
\end{block}

\begin{center} \includegraphics[width=2in]{images/transition.png} \end{center}

\end{frame}

%%

\begin{frame}{Introduction}

\begin{block}{Policy \cite{puterman2014markov}}
\begin{itemize}
\item A \textit{policy} $\pi$ is a function $\pi: A \times S \to [0, 1]$ where $\pi(a \mid s)$ is the probability of taking action $a$ in state $s$.
\item A \textit{deterministic policy} $\pi$ is a function $\pi: S \to A$ where $\pi(s)$ is the action taken in state $s$.
\end{itemize}
\end{block}

\begin{block}{Discounted Return \cite{puterman2014markov}}
\begin{itemize}
\item The \textit{discounted return} $G_t$ at time $t$ is the sum of all future (discounted) rewards. That is, 
$$G_{t} = \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}.$$
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{Introduction}

\begin{block}{State-Action Value Function \cite{puterman2014markov}}
\begin{itemize}
\item The \textit{state-action value function} $Q_{\pi}: S \times A \to \mathbb{R}$ is the expected discounted return of taking action $a$ in state $s$ and following policy $\pi$ thereafter. That is,
$$Q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ G_{t} \mid s_{t} = s, a_{t} = a \right].$$
\end{itemize}
\end{block}

\begin{block}{Optimal Policy \cite{puterman2014markov}}
\begin{itemize}
\item An \textit{optimal policy} $\pi^{*}$ is a policy that maximizes the state-action value function such that 
$$\pi^{*} = \arg \max_{\pi} Q_{\pi}(s, a)$$
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{Methods}

\begin{block}{RISK (Game) \cite{risk1959}}
\begin{itemize}
\item We described a simplified 2-player version of RISK.
\item There are 3 continents $\{0, 1, 2, 3\}$, $\{4, 5\}$, and $\{6, 7\}$.
\item Each territory must always be occupied by at least 1 army.
\item A turn has 4 phases: reinforce, attack, fortify, and end turn.
\end{itemize}
\end{block}

\begin{center} \includegraphics[width=1.5in]{images/graph.png} \end{center}

\end{frame}

%%

\begin{frame}{Methods}

\begin{block}{Reinforce \cite{risk1959}}
\begin{itemize}
\item If player 1 controls $t$ territories and has continent bonus $c$, then he receives $\min(\lfloor t/3\rfloor + c, 1)$ armies.
\item He may distribute these armies anywhere in his territories.
\end{itemize}
\end{block}

\begin{block}{Attack \cite{risk1959}}
\begin{itemize}
\item Player 1 commits $k$ armies to attack an adjacent territory with $m$ armies controlled by player 2.
\item Player 1 rolls $\min(k, 3)$ dice and player 2 rolls $\min(m, 2)$ dice.
\item The highest die of each player is compared and the second highest die of each player is compared. Player 2 wins ties.
\item For each die lost, the player loses one army.
\item If victorious, player 1 may attack again with his remaining troops.
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{Methods}

\begin{block}{Fortify \cite{risk1959}}
\begin{itemize}
\item Player 1 may redistribute his armies.
\item He may only move armies through his contiguous territories.
\end{itemize}
\end{block} 

\begin{block}{End Turn}
\begin{itemize}
\item Player 1 passes the turn to player 2.
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{Methods}

\begin{block}{Environment \cite{towers2024gymnasium}}
\begin{itemize}
\item The environment was implemented with Gymnasium.
\item The \textit{state space} $S$ was 
$$S = (\{1, 2\} \times \{1, \ldots, 40\})^{8}$$
where $s = (x_{1}, y_{1}, \ldots, x_{8}, y_{8})$ means that player $x_{i}$ controls $y_{i}$ armies in territory $i$.
\item The \textit{action space} $A$ was the set of all legal moves in 4 phases: reinforce, attack, fortify, and end turn.
\item The \textit{transition function} $P$ was defined by the combat mechanics.
\end{itemize}
\end{block}
  
\end{frame}

%%

\begin{frame}{Methods}

\begin{block}{Environment}
\begin{itemize}
\item The \textit{reward function} $R$ was based on the difference between the reinforcement values of the two players.
\item Intermediate rewards were scaled by a factor of $0.1$.
\item The \textit{discount factor} was $\gamma = 0.99$.
\end{itemize}
\end{block}
  
\end{frame}

%%

\begin{frame}{Methods}

\begin{block}{Reinforcement Learning \cite{chollet2015keras} \cite{puterman2014markov} \cite{van2016deep}}
\begin{itemize}
\item Double deep Q-Learning was implemented with Keras.
\item An epsilon-greedy policy was used.
\item The agent learned to play against a random opponent.
\item 256 games of 20 turns were played.
\item Starting positions were randomly generated.
\end{itemize}
\end{block}

\begin{block}{Test}
\begin{itemize}
\item The agent played 32 games of 20 turns against a random opponent.
\item The victor and final reward were recorded.
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{Results}

\begin{block}{Random versus Random}
\begin{itemize}
\item $42.19\%$ win rate
\item $-0.1875$ average reward.
\end{itemize}
\end{block}

\begin{block}{Agent versus Random}
\begin{itemize}
\item $57.81\%$ win rate
\item $+0.0893$ average reward.
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{Results}

\begin{center} \includegraphics[width=4in]{images/results.png} \end{center}

\end{frame}

%%

\begin{frame}{Discussion}

\begin{block}{Sun Tzu said... \cite{sun_tzu_art_of_war_132}}
\begin{itemize}
\item "So in war, the way is to avoid what is strong, and strike at what is weak."
\item "Do not swallow a bait offered by the enemy."
\end{itemize}
\end{block}

\begin{center} \includegraphics[width=4in]{images/discussion1.png} \end{center}

\end{frame}

%%

\begin{frame}{Discussion}

\begin{block}{Sun Tzu said... \cite{sun_tzu_art_of_war_132}}
\begin{itemize}
\item "Move only if there is a real advantage to be gained."
\item "Whether to concentrate or to divide your troops, must be decided by circumstances."
\end{itemize}
\end{block}

\begin{center} \includegraphics[width=4in]{images/discussion2.png} \end{center}

\end{frame}

%%

\begin{frame}{Conclusion}

\begin{block}{Conclusion}
\begin{itemize}
\item Our MDP + Double DQN method yielded modest improvements over a random agent.
\item Still, the agent would lose against a heuristic agent or a human player.
\end{itemize}
\end{block}

\begin{block}{Sun Tzu said... \cite{sun_tzu_art_of_war_132}}
\begin{itemize}
\item “To know your Enemy, you must become your Enemy.”
\item Train by self-play.
\end{itemize}
\end{block}

\end{frame}

%%

\begin{frame}{References}

\bibliographystyle{apalike}
\bibliography{refs}

\end{frame}

%%

\begin{frame}{Appendix}

\begin{block}{Code Availability}
\begin{itemize}
\item \href{https://github.com/dyao13/risk\_agent}{github.com/dyao13/risk\_agent}
\end{itemize}
\end{block}

\end{frame}

\end{document}